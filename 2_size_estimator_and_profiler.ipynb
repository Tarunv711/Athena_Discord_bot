{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarunv711/Athena_Discord_bot/blob/main/2_size_estimator_and_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl1A-uzhwpt_"
      },
      "source": [
        "# **2. Model Size Estimation**\n",
        "\n",
        "It is no surprise that with such a tiny package, your Ardunio Nano 33 BLE Sense comes with limited memory and processing power. Therefore, you must be aware of the size and components of your model in order to have it run efficiently on your MCU.\n",
        "\n",
        "This notebook explores how various neural network layers affect the number of parameters, the amount memory, the number of floating point operations, and the CPU runtime of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 2.0 Setup GDrive and Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BgbZjaQZ8niT",
        "outputId": "6148704e-1295-49e6-afb0-2b0498577eb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sH-xe50YtFNd"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rRj2U4Y3ttgu",
        "outputId": "5d5ff1c6-7a09-4b2e-f106-54c87f79d472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "Cloning into 'a2-YOUR_HANDLE'...\n",
            "fatal: could not read Password for 'https://YOUR_TOKEN@github.com': No such device or address\n",
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/ece5545/a2-YOUR_HANDLE'\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R9V2uP4YtzuR"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIzgKXcDtFNe"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ePTgb55gwT7o",
        "outputId": "b8375b23-94d4-497c-eb85-272e65f92bba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a2-YOUR_HANDLE\n",
            "ls: cannot access '/content/gdrive/MyDrive/ece5545/a2-YOUR_HANDLE/src': No such file or directory\n",
            "['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(PROJECT_ROOT)\n",
        "!ls {PROJECT_ROOT}/src\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2uKEHl-OtFNf",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "7e3a64f8-b705-41ee-a661-2dd767eb07f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a26506075805>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_proc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys,os\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nnt\n",
        "import src.data_proc as data_proc\n",
        "from src.constants import *\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imported code dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTKG-QIfwcN9"
      },
      "source": [
        "## 2.2 Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRSPvUTyIQ2"
      },
      "source": [
        "### Create the model\n",
        "Our TinyConv model currently consists of 7 layers:\n",
        "\n",
        "\n",
        "1. [Reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "2. [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "3. [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "4. [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
        "5. Reshape\n",
        "6. [Fully Connected (Linear)](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "7. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)\n",
        "\n",
        "\n",
        "Please refer to `<github_dir>/src/networks.py` for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU2ryBhlwcN_"
      },
      "outputs": [],
      "source": [
        "# Define device\n",
        "from src.networks import TinyConv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLwsdGt_liKa"
      },
      "source": [
        "### Create data_proc.AudioProcessor() object for data preprocessing\n",
        "When an AudioProcessor instance is created:\n",
        "\n",
        "1. Download speech_command dataset from DATA_URL (defined in constants.py) to data_dir (default: '/content/gdrive/MyDrive/ece5545/data')\n",
        "default dataset url: 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "\n",
        "2. Determine classes and their numerical indices for training and testing based on WANTED_WORDS\n",
        "(defined in constants.py):\n",
        "eg. if WANTED_WORDS is ['yes', 'no'], model will be trained to identify \"yes\" and \"no\" as yes and no,\n",
        "other words as unkown, and background noises as silence\n",
        "\n",
        "3. Determine and save the settings for data processing feature generator based on relavent constants\n",
        "in constants.py\n",
        "\n",
        "4. Determine which audio files in the dataset are for testing, training, or validating using hash method\n",
        "\n",
        "5. Prepare and save background noise data using the background noise data inside dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLxcu6BB_wxg"
      },
      "outputs": [],
      "source": [
        "# Create audio processor (this takes some time the first time)\n",
        "# And continues to run for a bit after reaching 100% while it's extracting files\n",
        "audio_processor = data_proc.AudioProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RJ8a0otJEJp"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model_fp32 = TinyConv(audio_processor.model_settings)\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcB7Ri8ewcOA"
      },
      "source": [
        "## 2.3 Model Estimates\n",
        "Run the next few cells to see how each layer impacts memory and runtime of the below TinyConv neural network model. Then experiment with reshaping it to see how adding or removing layers alters the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEnwhmThwcOC"
      },
      "source": [
        "### Memory Utilization\n",
        "\n",
        "There are two important forms of memory that we care about for MCUs: **flash memory** and **random access memory (RAM)**. Flash is **non-volatile** aka persistent storage memory; its data is saved when powered off. This is where your model's weights and code live, thus they must be able to fit within the capacity of your MCU's flash memory (1MB). On the other hand, RAM is **volatile** or non-persistent memory, thus it is used for temporary storage like input buffers and intermediate tensors. Together, they cannot exceed the size of your RAM storage (256KB).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Gv8LHUtFNi"
      },
      "source": [
        "### TODO 1: Implement the `count_trainable_parameters` function in `src/size_estimate.py` to compute model size and get an estimate of the flash usage of this model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--J01LrjtFNi"
      },
      "outputs": [],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import count_trainable_parameters\n",
        "num_params = count_trainable_parameters(model_fp32)\n",
        "print(\"Total number of trainable parameters: \", num_params / float(1e6), \"M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZaeZSLrtFNi"
      },
      "source": [
        "### TODO 2: Implement the `compute_forward_size` function in `src/size_estimate.py` to compute the memory needed for a forward pass. This is how much RAM you will be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLJ-LBfo56IL"
      },
      "outputs": [],
      "source": [
        "# Sends model weights to the GPU if tensors are on GPU\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "from src.size_estimate import compute_forward_memory\n",
        "frd_memory = compute_forward_memory(\n",
        "    model_fp32,\n",
        "    (1, model_fp32.model_settings['fingerprint_width'], model_fp32.model_settings['spectrogram_length']),\n",
        "    device\n",
        ")\n",
        "print(\"Forward memory: \", frd_memory / float(1e6), \"M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjm2OlXgArr"
      },
      "source": [
        "As you can see above, the number of parameters in a neural network can add up fast which is a concern when dealing with a small amount of RAM. With the TinyConv neural network only consuming 0.21MB out of 1MB, our model will easily fit within flash memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qkx4L8hCBD"
      },
      "source": [
        "### Number of Operations\n",
        "\n",
        "### TODO 3: Implement the `flop` function in `src/size_estimate.py` to count the total FLOPS in a forward pass with batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb5ugZA3wcOD"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from src.size_estimate import flop\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_fp32.cuda()\n",
        "\n",
        "# The total number of floating point operations\n",
        "flop_by_layers = flop(\n",
        "    model=model_fp32,\n",
        "    input_shape=(\n",
        "        1,\n",
        "        model_fp32.model_settings['fingerprint_width'],\n",
        "        model_fp32.model_settings['spectrogram_length']\n",
        "    ),\n",
        "    device=device)\n",
        "total_param_flops = sum([sum(val.values()) for val in flop_by_layers.values()])\n",
        "\n",
        "\n",
        "print(f'total number of floating operations: {total_param_flops}')\n",
        "print('Number of FLOPs by layer and parameters:')\n",
        "print(\"Conv: \", flop_by_layers['conv'])\n",
        "print(\"FC:   \", flop_by_layers['fc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAbpr4Mj12i"
      },
      "source": [
        "### CPU runtime\n",
        "\n",
        "### TODO 4: Measure the server/desktop CPU runtime to compare to the MCU runtime later in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMcAtzKHwcOD"
      },
      "outputs": [],
      "source": [
        "model_fp32.cpu()\n",
        "model_fp32.eval()\n",
        "inputs = torch.rand([1,1960]).cpu()\n",
        "\n",
        "# Run a profiler to see the cpu time for inference\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, with_flops=True, with_stack=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model_fp32(inputs)\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_size_estimator_and_profiler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "92bf126df007708fd70c442c808ee74575bedf7ea6317e0b182c3af0184af25d"
    },
    "kernelspec": {
      "display_name": "ece5545",
      "language": "python",
      "name": "ece5545"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}